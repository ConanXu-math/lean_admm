{"id": "0151b680-0846-4d1c-af27-9e0af9b45ad1", "code": "# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.2, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=2.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n    # Adjust mu to be more adaptive: as iterations increase, make threshold tighter\n    # Also consider the current rho to avoid large jumps when rho is already large\n    adjusted_mu = mu * (1.0 + 1.0 / (k + 2.0))  # Starts higher, decreases towards mu\n    \n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Scale step size based on rho to prevent overly aggressive updates when rho is large\n    # Use a damping factor that depends on log(1 + rho) to keep it bounded\n    damping = 1.0 / (1.0 + np.log1p(rho) / 10.0)\n    scaled_t = t * damping\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > adjusted_mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + scaled_t)\n        mode = \"mul\"\n\n    elif s_norm > adjusted_mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + scaled_t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: scaled tau_k (with damping)\n    aux = scaled_t\n\n    return new_rho, aux, mode\n", "language": "python", "parent_id": "47d01114-c051-4626-86a8-6070ff80c3ac", "generation": 2, "timestamp": 1770451145.319754, "iteration_found": 10, "metrics": {"combined_score": 0.038461538461538464, "metrics": {"converged": true, "iters": 13, "combined_score": 0.038461538461538464}, "artifacts": {"status": "CONVERGED", "iterations": 13, "eval_time": "22.446s"}, "formal_certification": "Lean4_Not_Auto_Proven"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 36 lines with 44 lines", "parent_metrics": {"combined_score": 0.07692307692307693, "metrics": {"converged": true, "iters": 13, "combined_score": 0.07692307692307693}, "artifacts": {"status": "CONVERGED", "iterations": 13, "eval_time": "21.675s"}, "formal_certification": "Lean4_Auto_Proven"}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert software developer tasked with iteratively improving a codebase.\nYour goal is to maximize the FITNESS SCORE while exploring diverse solutions across feature dimensions.\nThe system maintains a collection of diverse programs - both high fitness AND diversity are valuable.", "user": "# Current Program Information\n- Fitness: 0.0769\n- Feature coordinates: \n- Focus areas: - Fitness unchanged at 0.0769\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Metrics: combined_score: 0.0769, metrics: {'converged': True, 'iters': 13, 'combined_score': 0.07692307692307693}, artifacts: {'status': 'CONVERGED', 'iterations': 13, 'eval_time': '20.735s'}, formal_certification: Lean4_Auto_Proven\n- Outcome: Improvement in all metrics\n\n### Attempt 2\n- Changes: Change 1: Replace 36 lines with 44 lines\nChange 2: Replace 3 lines with 4 lines\n- Metrics: combined_score: 0.0769, metrics: {'converged': True, 'iters': 13, 'combined_score': 0.07692307692307693}, artifacts: {'status': 'CONVERGED', 'iterations': 13, 'eval_time': '21.544s'}, formal_certification: Lean4_Auto_Proven\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 36 lines with 36 lines\nChange 2: Replace 10 lines with 10 lines\n- Metrics: combined_score: 0.0769, metrics: {'converged': True, 'iters': 13, 'combined_score': 0.07692307692307693}, artifacts: {'status': 'CONVERGED', 'iterations': 13, 'eval_time': '21.675s'}, formal_certification: Lean4_Auto_Proven\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0769)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.2, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=2.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n\n```\nKey features: Performs well on combined_score (0.0769), Performs well on metrics ({'converged': True, 'iters': 13, 'combined_score': 0.07692307692307693}), Performs well on artifacts ({'status': 'CONVERGED', 'iterations': 13, 'eval_time': '21.675s'}), Performs well on formal_certification (Lean4_Auto_Proven)\n\n### Program 2 (Score: 0.0769)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n    # Adjust mu to decrease with iterations for finer convergence\n    # As k increases, mu approaches 1.0, making the condition stricter\n    # Start at mu and approach 1.0\n    adjusted_mu = 1.0 + (mu - 1.0) / (k + 1.0)\n    \n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Protect against division by zero\n    # Use max with eps only when necessary\n    r_protected = max(r_norm, eps)\n    s_protected = max(s_norm, eps)\n    \n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > adjusted_mu * s_protected:\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > adjusted_mu * r_protected:\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n\n```\nKey features: Performs well on combined_score (0.0769), Performs well on metrics ({'converged': True, 'iters': 13, 'combined_score': 0.07692307692307693}), Performs well on artifacts ({'status': 'CONVERGED', 'iterations': 13, 'eval_time': '21.544s'}), Performs well on formal_certification (Lean4_Auto_Proven)\n\n### Program 3 (Score: 0.0769)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n\n```\nKey features: Performs well on combined_score (0.0769), Performs well on metrics ({'converged': True, 'iters': 13, 'combined_score': 0.07692307692307693}), Performs well on artifacts ({'status': 'CONVERGED', 'iterations': 13, 'eval_time': '20.735s'}), Performs well on formal_certification (Lean4_Auto_Proven)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.0385)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Adjust mu based on iteration to be more aggressive early and conservative later\n    adaptive_mu = mu * (1.0 + 2.0 / (k + 2.0))\n    \n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > adaptive_mu * max(s_norm, eps):\n        # Use a larger step when far from balance\n        new_rho = rho * (1.0 + 2.0 * t)\n        mode = \"mul\"\n\n    elif s_norm > adaptive_mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + 2.0 * t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: capture update magnitude and direction\n    if mode == \"mul\":\n        aux = 2.0 * t\n    elif mode == \"div\":\n        aux = -2.0 * t\n    else:\n        aux = 0.0\n\n    return new_rho, aux, mode\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n### Program D2 (Score: 0.0417)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence with faster initial decay.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    # Add a small constant to prevent division by zero\n    # Adjust to decay faster initially\n    return c / ((k + 2.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Enhanced direction logic with balanced thresholds\n    # Compute ratio of residuals\n    ratio = r_norm / max(s_norm, eps)\n    \n    # More nuanced update conditions\n    if ratio > mu:\n        # Increase rho more aggressively when r_norm is much larger\n        new_rho = rho * (1.0 + 2.0 * t)\n        mode = \"mul\"\n        aux = ratio  # Store the ratio for information\n\n    elif ratio < 1.0 / mu:\n        # Decrease rho more aggressively when s_norm is much larger\n        new_rho = rho / (1.0 + 2.0 * t)\n        mode = \"div\"\n        aux = ratio\n\n    else:\n        # Keep rho but adjust slightly towards balance\n        # Small adjustment based on which residual is larger\n        if r_norm > s_norm:\n            new_rho = rho * (1.0 + 0.5 * t)\n            mode = \"adjust_up\"\n        elif s_norm > r_norm:\n            new_rho = rho / (1.0 + 0.5 * t)\n            mode = \"adjust_down\"\n        else:\n            new_rho = rho\n            mode = \"keep\"\n        aux = ratio\n\n    return new_rho, aux, mode\n\n```\nKey features: Alternative approach to combined_score, Alternative approach to metrics\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0769, Type: Exploratory)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], NumPy-based implementation, Comprehensive implementation\n\n### Inspiration 2 (Score: 0.0769, Type: Exploratory)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n    # Adjust mu to decrease with iterations for finer convergence\n    # As k increases, mu approaches 1.0, making the condition stricter\n    # Start at mu and approach 1.0\n    adjusted_mu = 1.0 + (mu - 1.0) / (k + 1.0)\n    \n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Protect against division by zero\n    # Use max with eps only when necessary\n    r_protected = max(r_norm, eps)\n    s_protected = max(s_norm, eps)\n    \n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > adjusted_mu * s_protected:\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > adjusted_mu * r_protected:\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], NumPy-based implementation\n\n### Inspiration 3 (Score: 0.0385, Type: Exploratory)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Adjust mu based on iteration to be more aggressive early and conservative later\n    adaptive_mu = mu * (1.0 + 2.0 / (k + 2.0))\n    \n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > adaptive_mu * max(s_norm, eps):\n        # Use a larger step when far from balance\n        new_rho = rho * (1.0 + 2.0 * t)\n        mode = \"mul\"\n\n    elif s_norm > adaptive_mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + 2.0 * t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: capture update magnitude and direction\n    if mode == \"mul\":\n        aux = 2.0 * t\n    elif mode == \"div\":\n        aux = -2.0 * t\n    else:\n        aux = 0.0\n\n    return new_rho, aux, mode\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], NumPy-based implementation\n\n# Current Program\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.2, p=1.2):\n    \"\"\"\n    Diminishing step size sequence.\n\n    Requirements:\n    - tau_k >= 0\n    - summable for p > 1\n    - depends only on k and fixed constants\n    \"\"\"\n    return c / ((k + 1.0) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=2.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current program, it's performing well with a fitness score of 0.0769, which matches the top performers. However, the system values diversity across features while maintaining or improving fitness. The current program is very similar to Program 1, which uses fixed parameters (mu=2.0, c=1.2 in tau). To explore a different approach while potentially maintaining or improving fitness, I can introduce a small variation that adjusts the update logic based on the iteration count, similar to Program 2 but with a different adjustment strategy.\n\nOne idea is to make the threshold mu adaptive in a different way: instead of linearly decreasing to 1.0, we can make it depend on the ratio of residuals to encourage more balanced updates. Also, we can adjust the step size multiplier based on how far the residuals are from balance.\n\nHowever, the current program is very concise (under 500 characters), which is good. Let's make a change that adds a small amount of complexity but remains within reasonable bounds.\n\nI'll modify the update_rho function to adjust mu based on the iteration and the current rho value, which could help in fine-tuning convergence. This approach is different from the top performers and may yield similar or better performance.\n\nHere's the change:\n\n<<<<<<< SEARCH\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=2.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n\n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + t)\n        mode = \"mul\"\n\n    elif s_norm > mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: purely tau_k (no residual dependence)\n    aux = t\n\n    return new_rho, aux, mode\n=======\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=2.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    C1-safe adaptive ADMM penalty update rule.\n\n    Conservative by design.\n    \"\"\"\n    # Adjust mu to be more adaptive: as iterations increase, make threshold tighter\n    # Also consider the current rho to avoid large jumps when rho is already large\n    adjusted_mu = mu * (1.0 + 1.0 / (k + 2.0))  # Starts higher, decreases towards mu\n    \n    # Summable step size (independent of residuals)\n    t = tau(k, c, p)\n\n    # Scale step size based on rho to prevent overly aggressive updates when rho is large\n    # Use a damping factor that depends on log(1 + rho) to keep it bounded\n    damping = 1.0 / (1.0 + np.log1p(rho) / 10.0)\n    scaled_t = t * damping\n\n    # Direction logic ONLY (may depend on residuals)\n    if r_norm > adjusted_mu * max(s_norm, eps):\n        new_rho = rho * (1.0 + scaled_t)\n        mode = \"mul\"\n\n    elif s_norm > adjusted_mu * max(r_norm, eps):\n        new_rho = rho / (1.0 + scaled_t)\n        mode = \"div\"\n\n    else:\n        new_rho = rho\n        mode = \"keep\"\n\n    # Auxiliary scalar: scaled tau_k (with damping)\n    aux = scaled_t\n\n    return new_rho, aux, mode\n>>>>>>> REPLACE\n\nThis change introduces two key features:\n1. An adjusted mu that starts slightly higher than the base mu and decreases towards it as iterations increase. This makes the update conditions slightly more lenient early on and tighter later, which could help in achieving better convergence.\n2. A damping factor on the step size based on the current rho value. When rho is large, updates become more conservative to avoid overshooting. This uses np.log1p(rho) which grows slowly, ensuring the damping is smooth.\n\nThese modifications add diversity to the feature space (complexity, adaptive behavior) while maintaining the core C1-safe property. The auxiliary scalar now includes the damping effect, which is a new feature not present in other programs.\n\nNote: This change increases the code length but remains focused. The use of np.log1p requires numpy, which is already imported. Let's ensure the function is still within the character limit.\n\nThis approach is different from the top performers and the diverse programs, offering a unique balance between adaptation and conservatism. It may lead to similar or improved fitness by providing more nuanced updates."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}